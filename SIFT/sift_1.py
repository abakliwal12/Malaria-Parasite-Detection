# -*- coding: utf-8 -*-
"""SIFT_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Z_sm2faEPmZ_yRX1UmADsS8oxW8Da6X2
"""

from google.colab import drive
drive.mount('/content/drive')

!unzip -q '/content/drive/My Drive/parasitedetection-iiitb2019.zip'

import os
import cv2
import copy
import csv
import random
import pickle
import numpy as np
import pandas as pd
import itertools
from scipy.stats import randint
from itertools import cycle
import matplotlib.pyplot as plt
from matplotlib.pyplot import figure
import ntpath
from sklearn import preprocessing
from sklearn.manifold import TSNE
from sklearn.decomposition import TruncatedSVD
from sklearn import metrics
from sklearn.model_selection import train_test_split
from sklearn import preprocessing
from scipy.sparse import csr_matrix
from scipy import stats
# from my_ml_lib import DataManipulationTools, MetricTools, PlotTools
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import RandomizedSearchCV
from sklearn.tree import DecisionTreeClassifier 
from sklearn import svm
from sklearn.neural_network import MLPClassifier
from xgboost import XGBClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, AdaBoostClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
from sklearn.decomposition import PCA
from skimage.feature import hog, local_binary_pattern

from sklearn import metrics
from sklearn.model_selection import train_test_split

def save_feature(feature, name):
    # saving all our feature vectors in pickled file
    with open('cache/' + name + '.pkl', 'wb') as fp:
        pickle.dump(csr_matrix(feature), fp)
    
    print(f'Feature saved with name cache/{name}.pkl')

def load_feature(feature_name):
    return pickle.load(open(feature_name, 'rb')).A

def resize_images(path, resize_image_path):
    # images = []
    img_paths = []
    labels = []
    img_names = []
    num1 = 64
    num2 = 64
    for file_name in os.listdir(path):
        # img_names.append(file_name)
        file_path = path + file_name + '/'
        resize_img_path = resize_image_path + '/' + file_name
        for img_name in os.listdir(file_path):
            if not img_name.startswith('.'):
                if img_name.endswith('.png'):
                    img_paths.append(img_name)
                    img_names.append(ntpath.basename(img_name))
                    img = cv2.imread(file_path + '/' + img_name)
                    new_img = cv2.resize(img, (num2, num1))

                    resize_img_path = resize_image_path + '/' + ntpath.basename(img_name)
                    cv2.imwrite(resize_img_path, new_img) 
                    # images.append(new_img)
                    if file_name == 'Parasitized':
                        label = 0
                    else:
                        label = 1
                    labels.append(label)
    
    return np.array(img_paths), np.array(labels), np.array(img_names)

!mkdir resized_images

cd resized_images

!mkdir train

cd train

!mkdir Parasitized

!mkdir Uninfected

resize_image_path = '/content/resized_images/train'
path = '/content/Parasite/Parasite/train/'
img_paths, labels, img_names = resize_images(path, resize_image_path)

os.listdir('/content/resized_images/train')

img_paths.shape, labels.shape, img_names.shape

img_paths

full_data = pd.DataFrame(columns=['Name', 'ImagePath', 'Label'])
full_data['Name'] = img_names
full_data['ImagePath'] = img_paths
full_data['Label'] = labels
# full_data.shape

append_parent_dir = lambda x : resize_image_path + '/' + x
full_data['ImagePath'] = full_data.ImagePath.apply(append_parent_dir)

def get_sift(images, name='sift'):
    img_path_list = list(images)
    # SIFT descriptor for 1 image
    def get_image_sift(image, vector_size=15):
        alg = cv2.xfeatures2d.SIFT_create()
        kps = alg.detect(image, None)
        kps = sorted(kps, key=lambda x: -x.response)[:vector_size]
        # Making descriptor of same size
        # Descriptor vector size is 128
        needed_size = (vector_size * 128)
        if len(kps) == 0:
            return np.zeros(needed_size)
        kps, dsc = alg.compute(image, kps)
        dsc = dsc.flatten()
        if dsc.size < needed_size:
            dsc = np.concatenate([dsc, np.zeros(needed_size - dsc.size)])
        return dsc
    # SIFT descriptor for all images
    features = []
    for img in img_path_list:
        image_arr = cv2.imread(img)
        dsc = get_image_sift(image_arr)
        features.append(dsc)
    result = np.array(features)
    return result

train_imgs, val_imgs, train_y, val_y = train_test_split(full_data[['Name', 'ImagePath']], full_data['Label'], test_size=0.2)

!pip install opencv-python==3.4.2.16
!pip install opencv-contrib-python==3.4.2.16

!pip install opencv-python==3.4.2.16
!pip install opencv-contrib-python==3.4.2.16

import cv2
sift_train = get_sift(train_imgs['ImagePath'], name='sift_train')
sift_val = get_sift(val_imgs['ImagePath'], name='sift_val')

sift_train.shape, sift_val.shape

# sift_train = load_feature('cache/sift_train.pkl')
# sift_val = load_feature('cache/sift_val.pkl')

def norm_features_zscore(train, test):
    min_max_scaler = preprocessing.StandardScaler()
    norm_train = min_max_scaler.fit_transform(train)
    norm_test = min_max_scaler.transform(test)
    
    return norm_train, norm_test, min_max_scaler

# norm_sift_train, norm_sift_val = norm_features_minmax(sift_train, sift_val)
norm_sift_train, norm_sift_val, min_max_scaler = norm_features_zscore(sift_train, sift_val)

pca = PCA(n_components=5)
pca_sift_train = pca.fit_transform(norm_sift_train)
pca_sift_val = pca.transform(norm_sift_val)

pca_sift_val.shape

def train_model(train_x, train_y, model_name='NB', validation=None):
    """
    Possible model names: ['NB', 'DT', 'RF']
    default = 'NB'
    
    validation: (val_x, val_y) tupple for validation accuracy score.
    
    return: trained model
    """
    model = None
    if model_name == 'DT':
        model = DecisionTreeClassifier()
    elif model_name == 'RF':
        model = RandomForestClassifier(n_estimators=200, max_depth=10)
    else:
        model = GaussianNB()
    
    model.fit(train_x, train_y)
    
    if validation is not None:
        y_hat = model.predict(validation[0])
        acc = metrics.accuracy_score(validation[1], y_hat)
        print(f"Validation Accuracy in '{model_name}' = {acc}")
        cm = metrics.confusion_matrix(validation[1], y_hat)
        print(cm)
        # plt.text(color='white')
        # plt.imshow(cm, interpolation='nearest', cmap=plt.get_cmap('Blues'))
        recall = cm[0][0] / (cm[0][0] + cm[0][1])
        precision = cm[0][0] / (cm[0][0] + cm[1][0])
        f1 = 2*(precision*recall)/(precision+recall)
        print(f"Recall in '{model_name}' = {recall}")
        print(f"Precision in '{model_name}' = {precision}")
        print(f"F1 Score in '{model_name}' = {f1}")
               
    return model

# np.save('cache/pca_sift_train.npy', pca_sift_train)
# np.save('cache/pca_sift_val.npy', pca_sift_val)

# pca_sift_train = np.load('cache/pca_sift_train.npy')
# pca_sift_val = np.load('cache/pca_sift_val.npy')

norm_sift_train.shape, norm_sift_val.shape

model1 = train_model(pca_sift_train, train_y, model_name='RF', validation=(pca_sift_val, val_y))

model2 = train_model(pca_sift_train, train_y, model_name='DT', validation=(pca_sift_val, val_y))

model3 = train_model(pca_sift_train, train_y, model_name='NB', validation=(pca_sift_val, val_y))

model_rf_pkl='/content/model_rf.pickle'
x=open(model_rf_pkl,'wb')
pickle.dump(model1,x)
x.close()
# sift_comp_train = combine_features([sift_train, sift_val], horizontal=False)

model_dt_pkl='/content/model_dt.pickle'
x=open(model_dt_pkl,'wb')
pickle.dump(model2,x)
x.close()

model_nb_pkl='/content/model_nb.pickle'
x=open(model_nb_pkl,'wb')
pickle.dump(model3,x)
x.close()

def resize_images_2(path, resize_image_path):
    # images = []
    img_paths = []
    labels = []
    img_names = []
    num1 = 64
    num2 = 64
    for img_name in os.listdir(path):
        file_path = path + '/' + img_name
        if not img_name.startswith('.'):
            if img_name.endswith('.png'):
                img_paths.append(img_name)
                img_names.append(ntpath.basename(img_name))
                img = cv2.imread(file_path)
                new_img = cv2.resize(img, (num2, num1))

                resize_img_path = resize_image_path + '/' + ntpath.basename(img_name)
                # print(resize_img_path)
                cv2.imwrite(resize_img_path, new_img) 
                # images.append(new_img)
                
    
    return np.array(img_paths), np.array(img_names)

os.listdir('/content/Parasite/Parasite/test')

resize_image_path = '/content/Parasite/Parasite/resized_images/test'
path = '/content/Parasite/Parasite/test'
img_paths, img_names = resize_images_2(path, resize_image_path)

len(img_paths), len(img_names)

#Enter Testing images set here i.e. text_x
#update it while running
test_x=[]
test_image_list = []
for img in os.listdir('/content/Parasite/Parasite/resized_images/test'):
    if '.png' in img:
        test_image_list.append(img)
        test_x.append(img)
sift_test = get_sift(test_x, name='sift_test')

sift_test.shape

# min_max_scaler = preprocessing.StandardScaler()
# norm_train = min_max_scaler.fit_transform(sift_train)
norm_test_2 = min_max_scaler.transform(sift_test)

# pca = PCA(n_components=5)
pca_sift_comp_test = pca.transform(norm_test_2)

pca_sift_comp_test.shape

model1 = None
with open('/content/model_rf.pickle', 'rb') as pickle_file:
    model1 = pickle.load(pickle_file)

# print(mode)

NB_Result = model3.predict(pca_sift_comp_test)

NB_Result

sub_df = pd.DataFrame(columns=['Name', 'Label'])
sub_df['Name'] = test_image_list
sub_df['Label'] = NB_Result
sub_df.to_csv('/content/sub_9.csv', index=False)

model2 = None
with open('/content/drive/My Drive/ML Project/SIFT_Model_Pickle_File/model_dt.pickle', 'rb') as pickle_file:
    model2 = pickle.load(pickle_file)

DT_Result = model2.predict(pca_sift_comp_test)

model3 = None
with open('/content/drive/My Drive/ML Project/SIFT_Model_Pickle_File/model_nb.pickle', 'rb') as pickle_file:
    model3 = pickle.load(pickle_file)

NB_Result = model3.predict(pca_sift_comp_test)